{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34rVf9SGL--I"
   },
   "source": [
    "# Initialize globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y5jf1ksbhM1t"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob, re\n",
    "\n",
    "project_path = 'C:/Users/vap43/Documents/GitHub/reddit_comment_classification/'\n",
    "TRAIN_DATA_PATH = project_path + \"data/data_train.pkl\"\n",
    "TEST_DATA_PATH = project_path + \"data/data_test.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5VRVSw0h5pP"
   },
   "source": [
    "# Import the text and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3EFl9T-igp8"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle(TRAIN_DATA_PATH)\n",
    "test_data  = pd.read_pickle(TEST_DATA_PATH)\n",
    "\n",
    "nb_X_Train = len(train_data[0])\n",
    "All_X = np.concatenate((np.array(train_data[0]),np.array(test_data)))\n",
    "y = np.array(train_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ewlSnlMdpM3a"
   },
   "source": [
    "# Pre-process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01kZ5H8ov2SI"
   },
   "source": [
    "Remove stop words and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9T9JISjh_QpX"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words_list = stopwords.words('english')\n",
    "pattern = re.compile(r'\\b\\w\\w+\\b')\n",
    "\n",
    "for idx, sentence in enumerate(All_X):\n",
    "  All_X[idx] = \" \".join([stemmer.stem(word) for word in re.findall(pattern, sentence.lower()) if word not in stop_words_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w_YrdTYfvywF"
   },
   "source": [
    "Count the terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qT-js4KyvObo"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "  ngram_range=(1, 1),\n",
    "  min_df=2,\n",
    "  max_df=1.0,\n",
    "  max_features=None,\n",
    ")\n",
    "All_X_Counts = vectorizer.fit_transform(All_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5cbjQsTkvxjy"
   },
   "source": [
    "Weight the terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uIoo9U7-vP_N"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "All_X_ifidf = tfidf_transformer.fit_transform(All_X_Counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-uaKq8IM4j0"
   },
   "source": [
    "Split out the Train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fecyI4aCuslL"
   },
   "outputs": [],
   "source": [
    "X = All_X_ifidf[:nb_X_Train,:]\n",
    "Kaggle_Test_X = All_X_ifidf[nb_X_Train:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U1iaUcwo4iSN"
   },
   "source": [
    "Onehot encode the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pFu-rKUaoDyo"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_onehot = onehot_encoder.fit_transform(y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BnFSfbhHeCob"
   },
   "source": [
    "\n",
    "# Create NN using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "9DWMw4LFeE3v",
    "outputId": "1cf7e484-9261-4872-8259-50e29c8159f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "70000/70000 [==============================] - 53s 754us/step - loss: 1.8131 - accuracy: 0.4997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1807fa04d08>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization, GaussianNoise\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.constraints import max_norm\n",
    "\n",
    "model = Sequential([\n",
    "  # Input layer\n",
    "  Dense(\n",
    "    1024,\n",
    "    use_bias=False,\n",
    "    input_shape=(X.shape[1],),\n",
    "    kernel_constraint=max_norm(3.),\n",
    "  ),\n",
    "  BatchNormalization(),\n",
    "  Activation('relu'),\n",
    "\n",
    "  # Output layer\n",
    "  Dense(\n",
    "    20,\n",
    "    use_bias=False\n",
    "  ),\n",
    "  BatchNormalization(),\n",
    "  Activation('softmax'),\n",
    "])\n",
    "\n",
    "model.compile(Adagrad(learning_rate=0.003),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y_onehot, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iIW3xGQLu5_9"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(Kaggle_Test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HK4me5_d73i8"
   },
   "source": [
    "# Write output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FBiRnGcN44ZI",
    "outputId": "692a59fe-f19f-4e71-f997-211a9b886776"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000,)\n"
     ]
    }
   ],
   "source": [
    "y_kaggle = onehot_encoder.inverse_transform(y_pred)\n",
    "y_kaggle = y_kaggle.squeeze()\n",
    "print(y_kaggle.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXdwcImjv9rD"
   },
   "outputs": [],
   "source": [
    "def create_and_save_submission(predictions, file_name=\"submission.csv\"):\n",
    "    ids = [i for i in range(len(predictions))]\n",
    "    sub_df = pd.DataFrame(data=list(zip(ids, predictions)), columns=[\"Id\",\"Category\"])\n",
    "    sub_df.to_csv(file_name, index=False)\n",
    "\n",
    "create_and_save_submission(y_kaggle)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text Classification Neural Net.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
